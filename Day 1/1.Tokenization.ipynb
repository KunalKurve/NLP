{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "- sent_tokenize -> Paragraph to Sentence\n",
    "- word_tokenize -> Sentence to Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "words = word_tokenize(sent)\n",
    "punctuations = list(string.punctuation)\n",
    "puncts = []\n",
    "for i in words:\n",
    "    if i in punctuations:\n",
    "        puncts.append(i)\n",
    "per_punc = (len(puncts)/len(words)) * 100\n",
    "per_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sent)\n",
    "text = [word for word in words if not word.isalnum()]\n",
    "punc_count = len(text)\n",
    "perc = punc_count / len(words) * 100\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 35)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('y'), ord('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µà¥€à¤µ'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = '\\u0935\\u0940\\u0935'\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à´Š'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = '\\u0D0A'\n",
    "char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à´Š'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x0D0A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'à¤®à¤¾à¤¨à¤¸à¥€ à¤®à¤¾à¤³à¤—à¥‡'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('\\u0935')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µà¤¨à¤¸à¥€ à¤µà¤³à¤—à¥‡'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.replace('à¤®à¤¾','\\u0935')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('à¤¨')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤•à¥à¤£à¤¾à¤² à¤•à¥à¤°à¥à¤µà¥‡', 'à¤•à¥à¤¨à¤¾à¤² à¤‰à¤•à¥‡', 'à¤®à¤¾à¤¨à¤¸à¥€ à¤®à¤¾à¤³à¤—à¥‡', 'à¤¯à¥‹à¤—à¥‡à¤¶ à¤¸à¤¿à¤°à¤¾à¤³']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastbenchers = ['à¤•à¥à¤¨à¤¾à¤² à¤‰à¤•à¥‡', 'à¤•à¥à¤£à¤¾à¤² à¤•à¥à¤°à¥à¤µà¥‡', 'à¤¯à¥‹à¤—à¥‡à¤¶ à¤¸à¤¿à¤°à¤¾à¤³', 'à¤®à¤¾à¤¨à¤¸à¥€ à¤®à¤¾à¤³à¤—à¥‡']\n",
    "lastbenchers.sort()\n",
    "lastbenchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤­à¤¾à¤°à¤¤',\n",
       " '(',\n",
       " 'à¤†à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤•',\n",
       " 'à¤¨à¤¾à¤®',\n",
       " ':',\n",
       " 'à¤­à¤¾à¤°à¤¤',\n",
       " 'à¤—à¤£à¤°à¤¾à¤œà¥à¤¯',\n",
       " ',',\n",
       " 'à¤…à¤‚à¤—à¥à¤°à¥‡à¤œà¤¼à¥€',\n",
       " ':',\n",
       " 'Republic',\n",
       " 'of',\n",
       " 'India',\n",
       " ',',\n",
       " 'à¤²à¤¿à¤ªà¥à¤¯à¤¨à¥à¤¤à¤°à¤£',\n",
       " ':',\n",
       " 'à¤°à¤¿à¤ªà¤¬à¥à¤²à¤¿à¤•',\n",
       " 'à¤‘à¤«à¤¼',\n",
       " 'à¤‡à¤‚à¤¡à¤¿à¤¯à¤¾',\n",
       " ')',\n",
       " 'à¤¦à¤•à¥à¤·à¤¿à¤£',\n",
       " 'à¤à¤¶à¤¿à¤¯à¤¾',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¸à¥à¤¥à¤¿à¤¤',\n",
       " 'à¤­à¤¾à¤°à¤¤à¥€à¤¯',\n",
       " 'à¤‰à¤ªà¤®à¤¹à¤¾à¤¦à¥à¤µà¥€à¤ª',\n",
       " 'à¤•à¤¾',\n",
       " 'à¤¸à¤¬à¤¸à¥‡',\n",
       " 'à¤¬à¤¡à¤¼à¤¾',\n",
       " 'à¤¦à¥‡à¤¶',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤­à¤¾à¤°à¤¤',\n",
       " 'à¤­à¥Œà¤—à¥‹à¤²à¤¿à¤•',\n",
       " 'à¤¦à¥ƒà¤·à¥à¤Ÿà¤¿',\n",
       " 'à¤¸à¥‡',\n",
       " 'à¤µà¤¿à¤¶à¥à¤µ',\n",
       " 'à¤•à¤¾',\n",
       " 'à¤¸à¤¾à¤¤à¤µà¤¾à¤',\n",
       " 'à¤¸à¤¬à¤¸à¥‡',\n",
       " 'à¤¬à¤¡à¤¼à¤¾',\n",
       " 'à¤¦à¥‡à¤¶',\n",
       " 'à¤¹à¥ˆ',\n",
       " ',',\n",
       " 'à¤œà¤¬à¤•à¤¿',\n",
       " 'à¤œà¤¨à¤¸à¤‚à¤–à¥à¤¯à¤¾',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤¦à¥ƒà¤·à¥à¤Ÿà¤¿à¤•à¥‹à¤£',\n",
       " 'à¤¸à¥‡',\n",
       " 'à¤¦à¥à¤¨à¤¿à¤¯à¤¾',\n",
       " 'à¤•à¤¾',\n",
       " 'à¤¸à¤¬à¤¸à¥‡',\n",
       " 'à¤¬à¤¡à¤¼à¤¾',\n",
       " 'à¤¦à¥‡à¤¶',\n",
       " 'à¤¹à¥ˆ',\n",
       " '[',\n",
       " '22',\n",
       " ']',\n",
       " 'à¥¤',\n",
       " 'à¤­à¤¾à¤°à¤¤',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤ªà¤¶à¥à¤šà¤¿à¤®',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤ªà¤¾à¤•à¤¿à¤¸à¥à¤¤à¤¾à¤¨',\n",
       " ',',\n",
       " 'à¤‰à¤¤à¥à¤¤à¤°',\n",
       " 'à¤ªà¤¶à¥à¤šà¤¿à¤®',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤…à¤«à¤—à¤¾à¤¨à¤¿à¤¸à¥à¤¤à¤¾à¤¨',\n",
       " ',',\n",
       " 'à¤‰à¤¤à¥à¤¤à¤°-à¤ªà¥‚à¤°à¥à¤µ',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤šà¥€à¤¨',\n",
       " ',',\n",
       " 'à¤¨à¥‡à¤ªà¤¾à¤²',\n",
       " 'à¤”à¤°',\n",
       " 'à¤­à¥‚à¤Ÿà¤¾à¤¨',\n",
       " ',',\n",
       " 'à¤ªà¥‚à¤°à¥à¤µ',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¬à¤¾à¤‚à¤—à¥à¤²à¤¾à¤¦à¥‡à¤¶',\n",
       " 'à¤”à¤°',\n",
       " 'à¤®à¥à¤¯à¤¾à¤¨à¥à¤®à¤¾à¤°',\n",
       " 'à¤¸à¥à¤¥à¤¿à¤¤',\n",
       " 'à¤¹à¥ˆà¤‚à¥¤',\n",
       " 'à¤­à¤¾à¤°à¤¤à¥€à¤¯',\n",
       " 'à¤®à¤¹à¤¾à¤¸à¤¾à¤—à¤°',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤‡à¤¸à¤•à¥‡',\n",
       " 'à¤¦à¤•à¥à¤·à¤¿à¤£',\n",
       " 'à¤ªà¤¶à¥à¤šà¤¿à¤®',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤®à¤¾à¤²à¤¦à¥€à¤µ',\n",
       " ',',\n",
       " 'à¤¦à¤•à¥à¤·à¤¿à¤£',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¶à¥à¤°à¥€à¤²à¤‚à¤•à¤¾',\n",
       " 'à¤”à¤°',\n",
       " 'à¤¦à¤•à¥à¤·à¤¿à¤£-à¤ªà¥‚à¤°à¥à¤µ',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤‡à¤‚à¤¡à¥‹à¤¨à¥‡à¤¶à¤¿à¤¯à¤¾',\n",
       " 'à¤¸à¥‡',\n",
       " 'à¤­à¤¾à¤°à¤¤',\n",
       " 'à¤•à¥€',\n",
       " 'à¤¸à¤¾à¤®à¥à¤¦à¥à¤°à¤¿à¤•',\n",
       " 'à¤¸à¥€à¤®à¤¾',\n",
       " 'à¤²à¤—à¤¤à¥€',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤‡à¤¸à¤•à¥‡',\n",
       " 'à¤‰à¤¤à¥à¤¤à¤°',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¹à¤¿à¤®à¤¾à¤²à¤¯',\n",
       " 'à¤ªà¤°à¥à¤µà¤¤',\n",
       " 'à¤¤à¤¥à¤¾',\n",
       " 'à¤¦à¤•à¥à¤·à¤¿à¤£',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤­à¤¾à¤°à¤¤à¥€à¤¯',\n",
       " 'à¤®à¤¹à¤¾à¤¸à¤¾à¤—à¤°',\n",
       " 'à¤¸à¥à¤¥à¤¿à¤¤',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤¦à¤•à¥à¤·à¤¿à¤£-à¤ªà¥‚à¤°à¥à¤µ',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¬à¤‚à¤—à¤¾à¤²',\n",
       " 'à¤•à¥€',\n",
       " 'à¤–à¤¾à¤¡à¤¼à¥€',\n",
       " 'à¤¤à¤¥à¤¾',\n",
       " 'à¤ªà¤¶à¥à¤šà¤¿à¤®',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤…à¤°à¤¬',\n",
       " 'à¤¸à¤¾à¤—à¤°',\n",
       " 'à¤¹à¥ˆà¥¤']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = '''à¤­à¤¾à¤°à¤¤ (à¤†à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤• à¤¨à¤¾à¤®: à¤­à¤¾à¤°à¤¤ à¤—à¤£à¤°à¤¾à¤œà¥à¤¯, à¤…à¤‚à¤—à¥à¤°à¥‡à¤œà¤¼à¥€: Republic of India, à¤²à¤¿à¤ªà¥à¤¯à¤¨à¥à¤¤à¤°à¤£: à¤°à¤¿à¤ªà¤¬à¥à¤²à¤¿à¤• à¤‘à¤«à¤¼ à¤‡à¤‚à¤¡à¤¿à¤¯à¤¾) à¤¦à¤•à¥à¤·à¤¿à¤£ à¤à¤¶à¤¿à¤¯à¤¾ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¿à¤¤ à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤‰à¤ªà¤®à¤¹à¤¾à¤¦à¥à¤µà¥€à¤ª à¤•à¤¾ à¤¸à¤¬à¤¸à¥‡ à¤¬à¤¡à¤¼à¤¾ à¤¦à¥‡à¤¶ à¤¹à¥ˆà¥¤ \n",
    "            à¤­à¤¾à¤°à¤¤ à¤­à¥Œà¤—à¥‹à¤²à¤¿à¤• à¤¦à¥ƒà¤·à¥à¤Ÿà¤¿ à¤¸à¥‡ à¤µà¤¿à¤¶à¥à¤µ à¤•à¤¾ à¤¸à¤¾à¤¤à¤µà¤¾à¤ à¤¸à¤¬à¤¸à¥‡ à¤¬à¤¡à¤¼à¤¾ à¤¦à¥‡à¤¶ à¤¹à¥ˆ, à¤œà¤¬à¤•à¤¿ à¤œà¤¨à¤¸à¤‚à¤–à¥à¤¯à¤¾ à¤•à¥‡ à¤¦à¥ƒà¤·à¥à¤Ÿà¤¿à¤•à¥‹à¤£ à¤¸à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤•à¤¾ à¤¸à¤¬à¤¸à¥‡ à¤¬à¤¡à¤¼à¤¾ à¤¦à¥‡à¤¶ à¤¹à¥ˆ[22]à¥¤ \n",
    "            à¤­à¤¾à¤°à¤¤ à¤•à¥‡ à¤ªà¤¶à¥à¤šà¤¿à¤® à¤®à¥‡à¤‚ à¤ªà¤¾à¤•à¤¿à¤¸à¥à¤¤à¤¾à¤¨, à¤‰à¤¤à¥à¤¤à¤° à¤ªà¤¶à¥à¤šà¤¿à¤® à¤®à¥‡à¤‚ à¤…à¤«à¤—à¤¾à¤¨à¤¿à¤¸à¥à¤¤à¤¾à¤¨, à¤‰à¤¤à¥à¤¤à¤°-à¤ªà¥‚à¤°à¥à¤µ à¤®à¥‡à¤‚ à¤šà¥€à¤¨, à¤¨à¥‡à¤ªà¤¾à¤² à¤”à¤° à¤­à¥‚à¤Ÿà¤¾à¤¨, à¤ªà¥‚à¤°à¥à¤µ à¤®à¥‡à¤‚ à¤¬à¤¾à¤‚à¤—à¥à¤²à¤¾à¤¦à¥‡à¤¶ à¤”à¤° à¤®à¥à¤¯à¤¾à¤¨à¥à¤®à¤¾à¤° à¤¸à¥à¤¥à¤¿à¤¤ à¤¹à¥ˆà¤‚à¥¤ \n",
    "            à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤®à¤¹à¤¾à¤¸à¤¾à¤—à¤° à¤®à¥‡à¤‚ à¤‡à¤¸à¤•à¥‡ à¤¦à¤•à¥à¤·à¤¿à¤£ à¤ªà¤¶à¥à¤šà¤¿à¤® à¤®à¥‡à¤‚ à¤®à¤¾à¤²à¤¦à¥€à¤µ, à¤¦à¤•à¥à¤·à¤¿à¤£ à¤®à¥‡à¤‚ à¤¶à¥à¤°à¥€à¤²à¤‚à¤•à¤¾ à¤”à¤° à¤¦à¤•à¥à¤·à¤¿à¤£-à¤ªà¥‚à¤°à¥à¤µ à¤®à¥‡à¤‚ à¤‡à¤‚à¤¡à¥‹à¤¨à¥‡à¤¶à¤¿à¤¯à¤¾ à¤¸à¥‡ à¤­à¤¾à¤°à¤¤ à¤•à¥€ à¤¸à¤¾à¤®à¥à¤¦à¥à¤°à¤¿à¤• à¤¸à¥€à¤®à¤¾ à¤²à¤—à¤¤à¥€ à¤¹à¥ˆà¥¤ \n",
    "            à¤‡à¤¸à¤•à¥‡ à¤‰à¤¤à¥à¤¤à¤° à¤®à¥‡à¤‚ à¤¹à¤¿à¤®à¤¾à¤²à¤¯ à¤ªà¤°à¥à¤µà¤¤ à¤¤à¤¥à¤¾ à¤¦à¤•à¥à¤·à¤¿à¤£ à¤®à¥‡à¤‚ à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤®à¤¹à¤¾à¤¸à¤¾à¤—à¤° à¤¸à¥à¤¥à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤¦à¤•à¥à¤·à¤¿à¤£-à¤ªà¥‚à¤°à¥à¤µ à¤®à¥‡à¤‚ à¤¬à¤‚à¤—à¤¾à¤² à¤•à¥€ à¤–à¤¾à¤¡à¤¼à¥€ à¤¤à¤¥à¤¾ à¤ªà¤¶à¥à¤šà¤¿à¤® à¤®à¥‡à¤‚ à¤…à¤°à¤¬ à¤¸à¤¾à¤—à¤° à¤¹à¥ˆà¥¤ '''\n",
    "para1 = sent_tokenize(para)\n",
    "words1 = word_tokenize(para)\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! How are you?\\n',\n",
       " 'Welcome to the world of Python Programming.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Hello Friends!\\tHow are you?\\n',\n",
       " 'Welcome to the world of Python\\tProgramming.\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"mydata.txt\",'r') as file:\n",
    "    # data = file.readable()  # Output: True\n",
    "    # data = file.readline()  # Output: 'Hello Friends! How are you?\\n'\n",
    "    data = file.readlines()  # Output: '['Hello Friends! How are you?\\n',\n",
    "                                        #'Welcome to the world of Python Programming.\\n']\n",
    "\n",
    "# words = word_tokenize(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.\\n\\n\\nHello',\n",
       " 'Friends!\\tHow',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python\\tProgramming.\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "with open(\"mydata.txt\",'r') as file:\n",
    "    data = file.read()      \n",
    "\n",
    "stk = SpaceTokenizer()\n",
    "\n",
    "stk.tokenize(data)\n",
    "# Split on single space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! How are you?\\nWelcome to the world of Python Programming.\\n\\n\\nHello Friends!',\n",
       " 'How are you?\\nWelcome to the world of Python',\n",
       " 'Programming.\\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "file = open(\"mydata.txt\") \n",
    "data = file.read()\n",
    "\n",
    "#create the object\n",
    "ttk = TabTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tab = ttk.tokenize(data)\n",
    "# Split on single tab\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! How are you?',\n",
       " 'Welcome to the world of Python Programming.',\n",
       " 'Hello Friends!\\tHow are you?',\n",
       " 'Welcome to the world of Python\\tProgramming.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "file = open(\"mydata.txt\") \n",
    "data = file.read()\n",
    "\n",
    "#create the object\n",
    "ltk = LineTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tab = ltk.tokenize(data)\n",
    "# Split on new line\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhiteSpace Tokenizer\n",
    "#### White space include -> Space, Tab and New Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.',\n",
       " 'Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "file = open(\"mydata.txt\") \n",
    "data = file.read()\n",
    "\n",
    "#create the object\n",
    "wtk = WhitespaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tab = wtk.tokenize(data)\n",
    "# Split on white spaces\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE Tokenizer: Multi Word Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Van Rossum is Python creator, visiting Pune this week. The development community is very eager the meet Van Rossum.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'the',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "sent1 = 'The Van Rossum is Python creator, visiting Pune this week. The development community is very eager the meet Van Rossum.'\n",
    "\n",
    "print(sent1)\n",
    "\n",
    "tab = word_tokenize(sent1)\n",
    "tab\n",
    "# We want Van Rossum together always. Hence use MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWETokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'the',\n",
       " 'meet',\n",
       " 'Van Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwet = MWETokenizer(separator=' ')\n",
    "mwet.add_mwe(('Van', 'Rossum'))\n",
    "\n",
    "# Tokenize the data\n",
    "tab = mwet.tokenize(tab)\n",
    "# Split on white spaces\n",
    "print(\"MWETokenizer\")\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends :)! How are you? Welcome to the world of NLP :D.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':',\n",
       " ')',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " ':',\n",
       " 'D',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "sent2 = 'Hello Friends :)! How are you? Welcome to the world of NLP :D.'\n",
    "\n",
    "print(sent2)\n",
    "\n",
    "tab = word_tokenize(sent2)\n",
    "tab\n",
    "# We want emojis together always. Hence use TweekT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " ':D',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttk = TweetTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "tab = ttk.tokenize(sent2)\n",
    "# Split on white spaces\n",
    "\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'FriendsðŸ˜‚',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ‘‹',\n",
       " '.',\n",
       " 'WelcomeðŸ™',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'ofðŸ’ðŸ»',\n",
       " 'Python',\n",
       " 'ProgrammingðŸ',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3 = 'Hello FriendsðŸ˜‚! How are you?ðŸ‘‹. WelcomeðŸ™ to the world ofðŸ’ðŸ» Python ProgrammingðŸ.'\n",
    "\n",
    "tab = word_tokenize(sent3)\n",
    "tab\n",
    "# We want emojis as different characters. It splits simply on space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " 'ðŸ˜‚',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ‘‹',\n",
       " '.',\n",
       " 'Welcome',\n",
       " 'ðŸ™',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'ðŸ’ðŸ»',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " 'ðŸ',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab = ttk.tokenize(sent3)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
