{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "- sent_tokenize -> Paragraph to Sentence\n",
    "- word_tokenize -> Sentence to Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "words = word_tokenize(sent)\n",
    "punctuations = list(string.punctuation)\n",
    "puncts = []\n",
    "for i in words:\n",
    "    if i in punctuations:\n",
    "        puncts.append(i)\n",
    "per_punc = (len(puncts)/len(words)) * 100\n",
    "per_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sent)\n",
    "text = [word for word in words if not word.isalnum()]\n",
    "punc_count = len(text)\n",
    "perc = punc_count / len(words) * 100\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 35)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('y'), ord('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'वीव'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = '\\u0935\\u0940\\u0935'\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ഊ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = '\\u0D0A'\n",
    "char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ഊ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x0D0A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'मानसी माळगे'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('\\u0935')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'वनसी वळगे'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.replace('मा','\\u0935')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('न')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['कुणाल कुर्वे', 'कुनाल उके', 'मानसी माळगे', 'योगेश सिराळ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastbenchers = ['कुनाल उके', 'कुणाल कुर्वे', 'योगेश सिराळ', 'मानसी माळगे']\n",
    "lastbenchers.sort()\n",
    "lastbenchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['भारत',\n",
       " '(',\n",
       " 'आधिकारिक',\n",
       " 'नाम',\n",
       " ':',\n",
       " 'भारत',\n",
       " 'गणराज्य',\n",
       " ',',\n",
       " 'अंग्रेज़ी',\n",
       " ':',\n",
       " 'Republic',\n",
       " 'of',\n",
       " 'India',\n",
       " ',',\n",
       " 'लिप्यन्तरण',\n",
       " ':',\n",
       " 'रिपब्लिक',\n",
       " 'ऑफ़',\n",
       " 'इंडिया',\n",
       " ')',\n",
       " 'दक्षिण',\n",
       " 'एशिया',\n",
       " 'में',\n",
       " 'स्थित',\n",
       " 'भारतीय',\n",
       " 'उपमहाद्वीप',\n",
       " 'का',\n",
       " 'सबसे',\n",
       " 'बड़ा',\n",
       " 'देश',\n",
       " 'है।',\n",
       " 'भारत',\n",
       " 'भौगोलिक',\n",
       " 'दृष्टि',\n",
       " 'से',\n",
       " 'विश्व',\n",
       " 'का',\n",
       " 'सातवाँ',\n",
       " 'सबसे',\n",
       " 'बड़ा',\n",
       " 'देश',\n",
       " 'है',\n",
       " ',',\n",
       " 'जबकि',\n",
       " 'जनसंख्या',\n",
       " 'के',\n",
       " 'दृष्टिकोण',\n",
       " 'से',\n",
       " 'दुनिया',\n",
       " 'का',\n",
       " 'सबसे',\n",
       " 'बड़ा',\n",
       " 'देश',\n",
       " 'है',\n",
       " '[',\n",
       " '22',\n",
       " ']',\n",
       " '।',\n",
       " 'भारत',\n",
       " 'के',\n",
       " 'पश्चिम',\n",
       " 'में',\n",
       " 'पाकिस्तान',\n",
       " ',',\n",
       " 'उत्तर',\n",
       " 'पश्चिम',\n",
       " 'में',\n",
       " 'अफगानिस्तान',\n",
       " ',',\n",
       " 'उत्तर-पूर्व',\n",
       " 'में',\n",
       " 'चीन',\n",
       " ',',\n",
       " 'नेपाल',\n",
       " 'और',\n",
       " 'भूटान',\n",
       " ',',\n",
       " 'पूर्व',\n",
       " 'में',\n",
       " 'बांग्लादेश',\n",
       " 'और',\n",
       " 'म्यान्मार',\n",
       " 'स्थित',\n",
       " 'हैं।',\n",
       " 'भारतीय',\n",
       " 'महासागर',\n",
       " 'में',\n",
       " 'इसके',\n",
       " 'दक्षिण',\n",
       " 'पश्चिम',\n",
       " 'में',\n",
       " 'मालदीव',\n",
       " ',',\n",
       " 'दक्षिण',\n",
       " 'में',\n",
       " 'श्रीलंका',\n",
       " 'और',\n",
       " 'दक्षिण-पूर्व',\n",
       " 'में',\n",
       " 'इंडोनेशिया',\n",
       " 'से',\n",
       " 'भारत',\n",
       " 'की',\n",
       " 'सामुद्रिक',\n",
       " 'सीमा',\n",
       " 'लगती',\n",
       " 'है।',\n",
       " 'इसके',\n",
       " 'उत्तर',\n",
       " 'में',\n",
       " 'हिमालय',\n",
       " 'पर्वत',\n",
       " 'तथा',\n",
       " 'दक्षिण',\n",
       " 'में',\n",
       " 'भारतीय',\n",
       " 'महासागर',\n",
       " 'स्थित',\n",
       " 'है।',\n",
       " 'दक्षिण-पूर्व',\n",
       " 'में',\n",
       " 'बंगाल',\n",
       " 'की',\n",
       " 'खाड़ी',\n",
       " 'तथा',\n",
       " 'पश्चिम',\n",
       " 'में',\n",
       " 'अरब',\n",
       " 'सागर',\n",
       " 'है।']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = '''भारत (आधिकारिक नाम: भारत गणराज्य, अंग्रेज़ी: Republic of India, लिप्यन्तरण: रिपब्लिक ऑफ़ इंडिया) दक्षिण एशिया में स्थित भारतीय उपमहाद्वीप का सबसे बड़ा देश है। \n",
    "            भारत भौगोलिक दृष्टि से विश्व का सातवाँ सबसे बड़ा देश है, जबकि जनसंख्या के दृष्टिकोण से दुनिया का सबसे बड़ा देश है[22]। \n",
    "            भारत के पश्चिम में पाकिस्तान, उत्तर पश्चिम में अफगानिस्तान, उत्तर-पूर्व में चीन, नेपाल और भूटान, पूर्व में बांग्लादेश और म्यान्मार स्थित हैं। \n",
    "            भारतीय महासागर में इसके दक्षिण पश्चिम में मालदीव, दक्षिण में श्रीलंका और दक्षिण-पूर्व में इंडोनेशिया से भारत की सामुद्रिक सीमा लगती है। \n",
    "            इसके उत्तर में हिमालय पर्वत तथा दक्षिण में भारतीय महासागर स्थित है। दक्षिण-पूर्व में बंगाल की खाड़ी तथा पश्चिम में अरब सागर है। '''\n",
    "para1 = sent_tokenize(para)\n",
    "words1 = word_tokenize(para)\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! How are you?\\n',\n",
       " 'Welcome to the world of Python Programming.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Hello Friends!\\tHow are you?\\n',\n",
       " 'Welcome to the world of Python\\tProgramming.\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"mydata.txt\",'r') as file:\n",
    "    # data = file.readable()  # Output: True\n",
    "    # data = file.readline()  # Output: 'Hello Friends! How are you?\\n'\n",
    "    data = file.readlines()  # Output: '['Hello Friends! How are you?\\n',\n",
    "                                        #'Welcome to the world of Python Programming.\\n']\n",
    "\n",
    "# words = word_tokenize(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.\\n\\n\\nHello',\n",
       " 'Friends!\\tHow',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python\\tProgramming.\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "with open(\"mydata.txt\",'r') as file:\n",
    "    data = file.read()      \n",
    "\n",
    "stk = SpaceTokenizer()\n",
    "\n",
    "stk.tokenize(data)\n",
    "# Split on single space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! How are you?\\nWelcome to the world of Python Programming.\\n\\n\\nHello Friends!',\n",
       " 'How are you?\\nWelcome to the world of Python',\n",
       " 'Programming.\\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "file = open(\"mydata.txt\") \n",
    "data = file.read()\n",
    "\n",
    "#create the object\n",
    "ttk = TabTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tab = ttk.tokenize(data)\n",
    "# Split on single tab\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! How are you?',\n",
       " 'Welcome to the world of Python Programming.',\n",
       " 'Hello Friends!\\tHow are you?',\n",
       " 'Welcome to the world of Python\\tProgramming.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "file = open(\"mydata.txt\") \n",
    "data = file.read()\n",
    "\n",
    "#create the object\n",
    "ltk = LineTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tab = ltk.tokenize(data)\n",
    "# Split on new line\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhiteSpace Tokenizer\n",
    "#### White space include -> Space, Tab and New Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.',\n",
       " 'Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "file = open(\"mydata.txt\") \n",
    "data = file.read()\n",
    "\n",
    "#create the object\n",
    "wtk = WhitespaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tab = wtk.tokenize(data)\n",
    "# Split on white spaces\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE Tokenizer: Multi Word Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Van Rossum is Python creator, visiting Pune this week. The development community is very eager the meet Van Rossum.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'the',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "sent1 = 'The Van Rossum is Python creator, visiting Pune this week. The development community is very eager the meet Van Rossum.'\n",
    "\n",
    "print(sent1)\n",
    "\n",
    "tab = word_tokenize(sent1)\n",
    "tab\n",
    "# We want Van Rossum together always. Hence use MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWETokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'the',\n",
       " 'meet',\n",
       " 'Van Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwet = MWETokenizer(separator=' ')\n",
    "mwet.add_mwe(('Van', 'Rossum'))\n",
    "\n",
    "# Tokenize the data\n",
    "tab = mwet.tokenize(tab)\n",
    "# Split on white spaces\n",
    "print(\"MWETokenizer\")\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends :)! How are you? Welcome to the world of NLP :D.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':',\n",
       " ')',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " ':',\n",
       " 'D',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "sent2 = 'Hello Friends :)! How are you? Welcome to the world of NLP :D.'\n",
    "\n",
    "print(sent2)\n",
    "\n",
    "tab = word_tokenize(sent2)\n",
    "tab\n",
    "# We want emojis together always. Hence use TweekT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " ':D',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttk = TweetTokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "tab = ttk.tokenize(sent2)\n",
    "# Split on white spaces\n",
    "\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends😂',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " '👋',\n",
       " '.',\n",
       " 'Welcome🙏',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of💁🏻',\n",
       " 'Python',\n",
       " 'Programming🐍',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3 = 'Hello Friends😂! How are you?👋. Welcome🙏 to the world of💁🏻 Python Programming🐍.'\n",
    "\n",
    "tab = word_tokenize(sent3)\n",
    "tab\n",
    "# We want emojis as different characters. It splits simply on space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '😂',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " '👋',\n",
       " '.',\n",
       " 'Welcome',\n",
       " '🙏',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " '💁🏻',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '🐍',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab = ttk.tokenize(sent3)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
