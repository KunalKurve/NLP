{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        # embed_dim : This parameter specifies the dimentionality of input\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "         # self ffn : This creates a feedforward network , often used for additional normalization\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim,activation='relu'),\n",
    "             Dense(embed_dim)]\n",
    "        )\n",
    "        #  these createLayerNormalization layers\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        # These create dropout layers, randomly sets\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self,inputs,training):\n",
    "        # Applies multi-head attention to the input sequence \n",
    "        attn_output = self.att(inputs,inputs)\n",
    "        \n",
    "        attn_output = self.dropout1(attn_output,training=training)\n",
    "\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "\n",
    "        ffn_output = self.dropout2(ffn_output,training=training)\n",
    "\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        # maxlen : The maximum length of the input_sequnces  the model will handle\n",
    "        # vocab_size : The total number of unique tokens (words) in the vocabulory \n",
    "        super().__init__()\n",
    "        # An Embedding layer that maps each token in the input sequence \n",
    "        # to a dense vector of size embed_dim\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        # An Embedding layer that maps each position in the sequence\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "    \n",
    "    def call(self,x):\n",
    "        # Extracts the actual length of the current input sequence\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        # Creates a tensor of positions from 0 to maxlen-1\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # Looks up the position embeddings for each element-wise\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        # Resulting in a combined representation that captures both words\n",
    "        # meaning and positional information\n",
    "        return x + positions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2000 # Only consider the 20k words\n",
    "maxlen = 200 #Only consider the first 200 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training Sequences\n",
      "25000  Validation Sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training Sequences\")\n",
    "print(len(x_val), \" Validation Sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 200), (25000, 200))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  89,    8,  511,    2,   59,    2,   41,  523,  147, 1876,    5,\n",
       "          2,  175,  347,   11,  618,    4,  172,   96,    2,    2,    9,\n",
       "        862,    2,    8,   41,    5,   27,  532,    2,    9,    2,    4,\n",
       "          2,  136,    2,    2,    5,    2,   19, 1456,  921,   42,    2,\n",
       "       1488,   68,    2,  216,   17,    6,    2,   48,   13,   69,    6,\n",
       "          2,   13,   62,   28,    2,   12,    8,   98,  634,  908,   10,\n",
       "         10,    2,    2,    9,    2,   17,    2,    6,   87, 1465,   48,\n",
       "         25,  377,   27,  478,  157,   11,    2,    2,   29,    2,    4,\n",
       "          2,    7,    2,    2,   83,    6,    2,    2,    7,  107,   42,\n",
       "        289,  715,  257,    5,   95,    2,    4,    2,   11,   17,    2,\n",
       "          5,    2, 1377,   17,  614,   11,   14,  365, 1652,    2,    2,\n",
       "        373,   10,   10,    4,  167,    2,    2,  287,   64,   35,    2,\n",
       "          2,    7, 1489,    4,  370,  121,   12,   80,  123,  178,   51,\n",
       "         75,  181,    8,   67,    4,  636,    2,    9,    2,    2,  190,\n",
       "         50,    9,  486,   54,   11,    6,  303,  548,    2,  684,    2,\n",
       "          2,  208,   11,    4,    2,    2,   95,    2,    4,    2,    2,\n",
       "        190,  122,   15,   79,  143,   10,   10, 1479, 1468,    9,    6,\n",
       "        196,  297,   14,  310,    9,   24, 1178,   18,    2,  361,   42,\n",
       "         76,  334])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all-together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embed_dim=32\n",
    "num_heads=2\n",
    "ff_dim=32\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 200, 32)           70400     \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block (Transfo  (None, 200, 32)           10656     \n",
      " rmerBlock)                                                      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81758 (319.37 KB)\n",
      "Trainable params: 81758 (319.37 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Administrator.DAI-PC2\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "782/782 [==============================] - 39s 46ms/step - loss: 0.4230 - accuracy: 0.7986 - val_loss: 0.3099 - val_accuracy: 0.8680\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2983 - accuracy: 0.8761 - val_loss: 0.3020 - val_accuracy: 0.8706\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.2772 - accuracy: 0.8855 - val_loss: 0.3054 - val_accuracy: 0.8650\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2673 - accuracy: 0.8897 - val_loss: 0.3045 - val_accuracy: 0.8673\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.2558 - accuracy: 0.8920 - val_loss: 0.3358 - val_accuracy: 0.8588\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.2489 - accuracy: 0.8935 - val_loss: 0.3232 - val_accuracy: 0.8644\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.2363 - accuracy: 0.8980 - val_loss: 0.3483 - val_accuracy: 0.8588\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2236 - accuracy: 0.8998 - val_loss: 0.3717 - val_accuracy: 0.8638\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2173 - accuracy: 0.8995 - val_loss: 0.4672 - val_accuracy: 0.8616\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2095 - accuracy: 0.9023 - val_loss: 0.3810 - val_accuracy: 0.8596\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,batch_size = 32,epochs=10,validation_data=(x_val,y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
